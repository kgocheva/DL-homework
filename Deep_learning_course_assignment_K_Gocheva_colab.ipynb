{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep learning course assignment K. Gocheva - colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kgocheva/DL-homework/blob/master/Deep_learning_course_assignment_K_Gocheva_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "-YoJWyry1a42",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Deep learning course Nov/Dec 2018\n",
        "## Course assignment of Kremena Gocheva\n",
        "\n",
        "## 1. Problem statement\n",
        "### 1.1 Context\n",
        "**The most common complaint of live scientists is about lacking or insufficient data. This is increasingly implausible in the age of big data and satellite imagery.** The actual issue is that data is collected sporadically and partially (e.g. when a project is funded for a few years and only covers butterflies to the exclusion of all other living things), stored in disjoined file formats or even offline, in some cases hoarded behind paywalls. We are in the process of transition from \"secret\" to open - and BIG - data and processing it in the old ways becomes extremely time consuming and expensive as the amount of data grows exponentially.\n",
        "\n",
        "\n",
        "In addition, **modern science is overly fragmented**. An enlightening analogy can be made to medicine - a knowledge based work for single practitioners only a few centuries ago but nowadays a flourishing industry to produce apparata, medicines and provide hospital care. As a result, the very first \"diagnostic\" task of the GP is to send the patient to the lab and examine a long list of indices from blood and urine samples, blood pressure meters, cardiograms etc. to diagnose single organs and systems while spending less and less time actually looking at the patient and answering the key question - is she healthy and if not, what's wrong? Much in the same manner, life sciences which used to be limited mostly to botany and zoology only a couple of centuries ago, now have branched out to cover all possible scales in space (from genetics to landscape and planetary ecology) and time (from paleobotanics or paleoecology to near real time tracking of species and remote sensing with drones and satellites). As a result, no single human is knowledgeable enough in all of these disciplines, and the majority of life scientists are increasingly ignorant about the advances in other areas of knowledge, including Machine learning and Artificial Intelligence. Such fragmentation also leads to difficulties in interdisciplinary communication and requires growing teams of narrow specialists. \n",
        "\n",
        "\n",
        "This increasingly intractable situation requires new approaches to data collection and processing, **in particular to the way we look at disjoined data from different sources**. If successful in uniting these data streams, longer data series could be produced and our knowledge would be largely enhanced. In addition, looking at the big picture and finding new patterns would be much easier as data from different sources (including remote sensing and various smart devices on the ground) is harnessed as input into AI algorithms. An illustration of the disruptive potential in ecology was given in the presentation by Gocheva, Apostolova, Sopotlieva, Velev, Vassilev, Radkov, Bratanova-Doncheva & Chipev, reproduced below. For more information see also the two presentations [here](https://drive.google.com/open?id=1RW8meQatB0tLYo7i2CEgDytwQ0odtEAu) and [here](https://drive.google.com/open?id=1xgME7iqJXanaWdaLs_EuDj9V4h476gLu), as well as  [this article](https://doi.org/10.1016/j.scitotenv.2018.11.192) \n",
        "\n",
        "Links to slide images: ! [Concept](https://drive.google.com/open?id=1eZL6neprMAQFrzI2t-eHvR4z6gzEWhqh) , ! [immediate tasks](https://drive.google.com/open?id=1IgOTCoYTWl6T4PLctdpVMuqeuj083O2l)  ,  [next steps](https://drive.google.com/open?id=10NPNPS66aGKcVu7f5s1PXszbIDXWxpXo)\n",
        "\n",
        "### 1.2 Envisaged pipeline\n",
        "\n",
        "The proposed tasks require running an ensemble of neural networks, ***of which the current assignment presents an early prototype for the first stage***. \n",
        "\n",
        "The envisaged workflow is:\n",
        "\n",
        "\n",
        "1.   Process each pixel of the preprocessed satellite data by feeding all bands into a CNN adapted from [here](https://www.hindawi.com/journals/js/2015/258619/) . The goal is to:\n",
        "\n",
        "\n",
        "*   learn a latent representation of the richer information contained in multiple bands\n",
        "*   reduce the dimensionality of the input bands to 3 (pseudo-R, pseudo-G and pseudo-B) for further input into pre-trained networks that expect RGB photo input.\n",
        "*   attempt a classification of the pixel based on Corine Landcover 2012 (CLC 12) (see [here](https://land.copernicus.eu/pan-european/corine-land-cover/clc-2012)). Later versions of the network can be trained via transfer learning to classify the pixel to a taxonomy of ecosystems or the more fine-grained taxonomy of habitats according to [this correspondence table](https://projects.eionet.europa.eu/eea-ecosystem-assessments/library/draft-ecosystem-map-europe/ecosystem-map-v2-1/crosswalk-maes-eunis-clc/download/en/1/crosswalk%20MAES-EUNIS-CLC.xlsx) (referred to as crosswalk), or future more precise High Resolution Layer products. CLC 12 has a minimum resolution of 100 m/pixel and pixels are classified into land cover categories with stated 85% accuracy. **This (topographic) accuracy will be assumsed to be \"human level\" performance for the model's assessment metrics during the test run.** While much preciser classification via Random Forest - achieving upward of 90% and even in some cases 95-96% - is a routine task and built in into the Copernicus SNAP software, the required field data is not easily available for processing at this moment.\n",
        "\n",
        "\n",
        "2.  Feed the pseudo - RGB generated in step 1. into a network for instance recognition which will have the task to create a latent representation of theterrain irregarding specifics such as cloud or light snow cover, mnor changes in land use and topography. The specific architecture will be searched out on the internet and probably be based on U networks. A data preprocessing pipeline will be set up on an optimally big server, possibly [RUS](https://rus-copernicus.eu/portal/) or [DIAS](https://www.copernicus.eu/en/access-data/dias) virtual machine. Due to resource constraints, priority in scene selection will be given to locations around [LTER Europe](http://www.lter-europe.net/lter-europe/about) sites where partners share various long term data series but not all LTER Europe sites measure the same environmental parameters or for the same time periods. \n",
        "\n",
        "3. Order the images collected for each location and pre-processed in steps 1. and 2. into a time series and experiment with supervised learning using different environmental parameters as well as unsupervised learning. This stage is not well conceptualised yet but will likely use both RNN and structure learning networks (e.g. capsule nets, [PGNNs](https://arxiv.org/abs/1710.11431)). The objective of this step is to determine which environmental parameters are the best indicators for ecosystem condition - a task that is far from trivial, see [here](https://drive.google.com/open?id=1FW9XUyqrL1r8fGhYSvQ1cXn2_wJYxGJB). \n",
        "\n",
        "4. Use the models best fitted for purpose from step 3. to predict ecosystem condition scenarios and derive the type and quantity of human uses (ecosystem services) they can provide depending on their location, size and condition. Ecosystem services production is largely unexplored yet due to the complex nature of some of the services (e.g. erosion protection requires a forest, pollination - a good habitat for wild bees and butterflies, etc.), or the way human intervention influences their production functions (e.g. there is a trade-off between cutting a wood to harvest timber - a provisioning ecosystem service, or keeping the wood for avalanche protection - a regulating ecosystem service) and I believe that the task can be facilitated by using deep learning models. If possible, comparison will be made with similar scale deterministic models run around some of the LTER Europe sites. \n",
        "\n",
        "***(for more informatrion on ecosystem services see [introduction in this excellent talk](https://youtu.be/gMem9QalY-o) and their classification in Europe - [CICES](https://cices.eu/resources/))***\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "YOR_g3_bQhL9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Assignment execution\n",
        "### 2.1 Data gathering and processing\n",
        "For his assignment, a limitred number of satellite image scenes would be sufficient since every pixel is a data unit.\n",
        "\n",
        "Data collected was aimed at covering all CLC classes; hence I selected and downloaded several scenes with minimal (<=1%) cloud coverage, in different locations where different land cover types and ecosystems are present. However, due to technical mishaps (The European Space Agency's data processing software [SNAP](http://step.esa.int/main/download/) was updated a few days before the assignment's due date and became very unstable and my less then stellar Python skills), only one of the scenes was processed in time; testing the neural network with two or more scenes from different satellites with varying number of bands became impossible for time constraints and will have to be performed at a later stage. \n",
        "\n",
        "It should be noted that some ecosystems (wetlands, sparcely vegetated rocky and dune landscapes, heathlands and shrubs) are underrepresented in Bulgaria and Europe. Therefore, even with more scenes added in future, the dataset will be unbalanced.\n",
        "\n",
        "The processing aims at achieving a matrix of the type \n",
        "\n",
        "[B1-1, B2-1, ... BN-1, CLC-1\n",
        "\n",
        "B1-2, B2-2, ... BN-2, CLC-2\n",
        "\n",
        "...\n",
        "\n",
        "B1-M, B2-M, ... BN-M, CLC-M] \n",
        "\n",
        "where B1...BN is the number of bands in the input, M is the number of pixels in the scene and CLC-1 to CLC-M are the Corine Landcover pixels in a scene cut out of the EU CLC dataset and having the same size as the satellite image scene.\n",
        "\n",
        "A key requirement for the success of the dataset composed as described above is the exact matching of all involved pixels (both from the satellite scene and the CLC layer) in size and position. This by necessity introduces uncertainty in the data since the minimum pixel size of the satellite bands is 10 m or below but within the same scene, some sensors work at a different scale and provide bigger size pixels than the others. In the scene used for this assignment (a Sentinel 2a product), the pixels for bands 1, 5, 6, 7, 8a, 9, 11 and 12 had to be downsized. For more information on sensor pixel sizes see the [ESA website](https://www.satimagingcorp.com/satellite-sensors/other-satellite-sensors/sentinel-2a/).  In addition, Corine Landcover 2012 is provided in 100 m pixels with a dominant land cover type. \n",
        "\n",
        "**Due to all these limitations and the significantly smaller number of bands as compared to the 220 bands per pixel in the original work, it can be expected that the model will not achieve high performance in classifying the landcover type. Its main merit will be the dimensionality reduction in the number of bands from N to 3 whereas the classification performance will at this stage merely be used for monitoring the model's training. **\n",
        "\n",
        "Initial processing (cutting out the scene from the satellite swath, georeferencing and producing the GeoTiff files for each band and CLC) was performed using SNAP and Quantum GIS. \n",
        "\n",
        "The resulting GeoTIFF for each band was processed into CSV using the following commands:"
      ]
    },
    {
      "metadata": {
        "id": "9MoLacK7baDA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from osgeo import gdal # The open source Geospatial Data Abstraction Library forming the interface to QGIS outputs\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "b1=gdal.open(D:\\b01.tif) # b01 is the variable for reading the Band 1 data. File location is spoofed, do not try to run the code!\n",
        "band01=np.array(b01.GetRasterBand(1).ReadAsArray())\n",
        "band01rav=band01.ravel()\n",
        "\n",
        "# ... repeat the sequence for b02 to b12 and corine GeoTIFF images. The remaining bands' values were situated in files with \n",
        "# filenames repeating the respective bands' nomenclature: b02, b03, b04, b05, b06, b07, b08, b8a, b09, b11, b12; the CLC \"ground truth\" \n",
        "# was in file corine.tif\n",
        "\n",
        "dataset = pd.concat([band01rav, band02rav, band03rav, band04rav, band05rav, band06rav, band07rav, band08rav, band8arav, band09rav, band11rav, band12rav], axis=1)\n",
        "\n",
        "# this resulted in a bunch of error messages so here my venture into data science ends... for now, until the re-exam :-)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7AV491NjeX0j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Example  outputs pasted from console:\n",
        "\n",
        "`>>> band01.shape`\n",
        "`(8982, 7900)`\n",
        "\n",
        "`>>> band01.size`\n",
        "`70957800`\n",
        "\n",
        "`>>> band01`\n",
        "\n",
        "`array([[1419, 1419, 1423, ..., 1498, 1630, 1630],`\n",
        "     `  [1419, 1419, 1423, ..., 1498, 1630, 1630],`\n",
        "      ` [1419, 1419, 1423, ..., 1498, 1630, 1630],`\n",
        "       ...,\n",
        "      ` [1353, 1353, 1352, ..., 1225, 1230, 1230],`\n",
        "      ` [1353, 1353, 1352, ..., 1225, 1230, 1230],`\n",
        "     `  [1353, 1353, 1352, ..., 1225, 1230, 1230]], dtype=uint16)`\n",
        "\n",
        "This scene yielded a dataset of 70 957 800 pixels which might be sufficient for training a relatively shallow CNN (the original paper is from 2015 and doesn't seem to have a very deep network)\n"
      ]
    },
    {
      "metadata": {
        "id": "HjKdp1fLUDf4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 Preparing the environment\n",
        "\n",
        "Due to my limited experience that apparently results in a sub-optimal Tensorflow installation, this turned to be a bit of a challenge that prevents me to have a training run at this time. "
      ]
    },
    {
      "metadata": {
        "id": "k_PNMOTtNFf2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Set up Tensorflow and Tensorboard (trickier, I tried to proceed as proposed [here](https://colab.research.google.com/drive/16YpqISWqruH6wQuyWFp5ZUi5RaIF0_nB#scrollTo=8lvWb8psoNpp) ). Unfortunately Tensorboard wouldn't run neither on my machine, nor on Colab yet ."
      ]
    },
    {
      "metadata": {
        "id": "1COkomv-fvgv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# supplementary libraries likely to be needed\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d5wFB3fVfvRS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# supplementary libraries likely to be needed\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z5IzaM9a1THR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "LOG_DIR = '/tmp/log'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JWs5xatXL_2a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "! curl http://localhost:6006"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vk8ywn3eNCot",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Install\n",
        "! npm install -g localtunnel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5OF9erFDUn1D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "... to be continued once the issues are resolved"
      ]
    },
    {
      "metadata": {
        "id": "Soe6CUzuj5pl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3 Creating the model\n",
        "\n",
        "This is more or less a dry run for now, more in the nature of jotting down the considerations behind the adopted design decisions than a concise, trainable model.\n",
        "\n",
        "First of all, the model **needs to accept different size inputs** since layer 1 from [this model](https://www.hindawi.com/journals/js/2015/258619/fig3/) ([code](https://www.hindawi.com/journals/js/2015/258619/alg1/)) described in [the original paper](https://www.hindawi.com/journals/js/2015/258619/) may have very different numbers of bands n, from just a few in radar satellite images, to over 200 in hyperspectral imagery. Therefore, in aberration from the original paper, the model will also have to be flexible in the number of convolutions performed on the inputs.\n",
        "\n",
        "Furthermore, the model must be functional and not seqential since we output pseudo-RGB AND a prediction of the pixel's landcover type (or in future iterations - the habitat/ecosystem type), as a control function. The desired output of pseudo-RGB values for the pixel determines the last fully connected layer's dimension of 3x1. \n",
        "\n",
        "The original model apparently features only four layers. At 220+ neurons each this may well have been testing the limits of 2013-2014 hardware. According to the original paper, however, the spectral characteristis of any evaluated pixel is key for prediction accuracy and some land cover types (such as different types of vegetation) may be difficult to distinguish if the number of channels were to drop from 200+ to 12-16 or below (see forest and grassland spectral signatures in [this figure](https://www.hindawi.com/journals/js/2015/258619/fig2/)). This is where, in my view, the model might be improved architectorally by increasing the number of convolutions, especially considering that data is abundant for there is no shortage of satellite image pixels. From the ecological point of view a more fine-grained reading of spectral information (especially coupled with improving satellite sensors in the upcoming space missions) may be good for distinguishing specifics such as ecosystem subtypes (e.g. dry from wet grasslands) or ecosystem condition (e.g. a healthy forest vs. a pest infested one)  This implementation follows the approach of [WaveNet](https://arxiv.org/pdf/1609.03499.pdf)) in the the convolutional filters look at different scales by using 1D convolution layers with causal padding and growing dilation. The attempt is made to mix the convolutional firlters'  with the ImageNet approach of concatenating different views.  \n",
        "\n",
        "The original paper favors a **tanh** activation and this approach was seconded at least in one paper of more recent origin on hyperspectral satellite image classification  that I came across, suggesting that there may be an architectural reason behind such a decision. However, there doesn't seem to be a TensorFlow implementation of tanh so the activation function is leaky relu instead. \n",
        "\n",
        "Causal padding was adopted following the information in the TF documentation."
      ]
    },
    {
      "metadata": {
        "id": "kpZG3ytrj4SA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "6c7cb903-bfb3-45c4-e362-9272c9442bc8"
      },
      "cell_type": "code",
      "source": [
        "bands=tf.get_variable( # Just in case the logic needs to distinguish between \n",
        "    # data entries or minibatches from several satellite products with different bands\n",
        "    \"bands\",\n",
        "    shape=None,\n",
        "    dtype=float16, #can be small since bands are a relatively small integer\n",
        "    initializer=RandomUniform,\n",
        "    regularizer=l2,\n",
        "    trainable=False, #This is supposed to be a classic style variable containing an extenally provided value\n",
        "    collections=None,\n",
        "    caching_device=None,\n",
        "    partitioner=None,\n",
        "    validate_shape=False, # The shape will vary with each input\n",
        "    use_resource=False, # Resource variables are beyond me at this stage :-)\n",
        "    custom_getter=None, # ** kwargs seem to be deprecated and generally  not sure, hence disabled\n",
        "    constraint=None, # Not sure so disabled\n",
        "    synchronization=tf.VariableSynchronization.AUTO, # No distributed training is envisaged for now\n",
        "    aggregation=tf.VariableAggregation.NONE # No distributed training is envisaged for now\n",
        ")\n",
        "\n",
        "model = models.Functional([\n",
        "    tf.keras.layers.Input(\n",
        "    shape=None, # flexible input size\n",
        "    batch_size=None, # To be found out experimentally depending on training curve and hardware, may be updated\n",
        "    name=In,\n",
        "    dtype=None,\n",
        "    sparse=False,\n",
        "    tensor=None,\n",
        "    )\n",
        "    \n",
        "    tf.keras.layers.Conv1D(# because the input is sequential although not exactly temporal. \n",
        "    #The original paper states that there's an analogy between time series and band value \n",
        "    #series, and we want to keep it this way\n",
        "        \n",
        "    name=\"No_Dilation_Convolution\"\n",
        "    filters=20, # As in the original paper. May need (upward) tuning\n",
        "    kernel_size=10, # keeping the balance between the wish to dilate and the possibly much lower number of channels. \n",
        "    #  May need modification to vary with dilation&stride\n",
        "    strides=1 # may or may not grow in each conv. layer, to be found during training\n",
        "    padding=causal, # Not temporal data but bands should be learned in order\n",
        "        # data_format or N/A, hence omitted\n",
        "    dilation_rate=1, # will grow in each conv. layer\n",
        "    activation=tf.keras.activations.relu(In, alpha=-0.01, max_value=None, threshold=0) # slightly leaky to safeguard against vanishing gradients \n",
        "        # although the network is not very deep. May need fine tuning\n",
        "    use_bias: False # to reduce decision points for starters, may need to change\n",
        "        # All remaining arguments I cannot decide on currently, hence omitted to keep defaults\n",
        "    )\n",
        "    \n",
        "    tf.keras.layers.Conv1D(   # mostly the same as the previous layer but with dilation=2     \n",
        "    name=\"Stride_Two_Dilated_Convolution\"\n",
        "    filters=20, \n",
        "    kernel_size=10, \n",
        "    strides=1 \n",
        "    padding=causal, \n",
        "    dilation_rate=2, \n",
        "    activation=tf.keras.activations.relu(In, alpha=-0.01, max_value=None, threshold=0) \n",
        "    use_bias: False\n",
        "    )\n",
        "    \n",
        "    tf.keras.layers.Conv1D(   # mostly the same as the previous layers but with dilation=4     \n",
        "    name=\"Stride_Four_Dilated_Convolution\"\n",
        "    filters=20, \n",
        "    kernel_size=10, \n",
        "    strides=2 \n",
        "    padding=causal, \n",
        "    dilation_rate=4, \n",
        "    activation=tf.keras.activations.relu(In, alpha=-0.01, max_value=None, threshold=0) \n",
        "    use_bias: False\n",
        "    )\n",
        "    \n",
        "    tf.keras.layers.Conv1D(   # mostly the same as the previous layers but with dilation=8     \n",
        "    name=\"Stride_Eight_Dilated_Convolution\"\n",
        "    filters=20, \n",
        "    kernel_size=10, \n",
        "    strides=3 \n",
        "    padding=causal, \n",
        "    dilation_rate=8, \n",
        "    activation=tf.keras.activations.relu(In, alpha=-0.01, max_value=None, threshold=0) \n",
        "    use_bias: False\n",
        "    )\n",
        "    \n",
        "    # No further exponential dilation seems feasible if 12-channeled pixels are to be processed\n",
        "    # No dropout layers implemented since data is very abundant\n",
        "    \n",
        "    tf.keras.layers.concatenate( # Emulate the ImageNet approach to bring all learned scales to one vector of latent representations\n",
        "    inputs=[\"No_Dilation_Convolution\", \"Stride_Two_Dilated_Convolution\", \"Stride_Four_Dilated_Convolution\", \"Stride_Eight_Dilated_Convolution\"],\n",
        "    axis=-1,\n",
        "    **kwargs\n",
        "    )\n",
        "\n",
        "    tf.keras.layers.Conv2D(   # Finally a \"normal\" convolution again     \n",
        "    name=\"Stride_Eight_Dilated_Convolution\"\n",
        "    filters=[36,3], \n",
        "    kernel_size=[3,1], # Not sure of the transition here between the 1D and MaxPooling layers\n",
        "    strides=1 \n",
        "    padding=valid, \n",
        "    dilation_rate=1, \n",
        "    activation=tf.keras.activations.relu(In, alpha=-0.01, max_value=None, threshold=0) \n",
        "    use_bias: False\n",
        "    )\n",
        "    \n",
        "# Here the model is supposed be condinurf by a Max-pooling followed by \n",
        "# several FCL's finishing at a 3 neurons layer to output the pseudo - RGB.\n",
        "# Then it is supposed to have two exits:\n",
        "#    - one from the last fully connected layer, outputting a 1x3 matrix representing the pixel's RGB values, and \n",
        "#    - one softmax classifying into the 44 Corine Landcover classes\n",
        "    \n",
        "    # (but I mixed up the dimensions and have to terminate before 14.00)\n",
        "    \n",
        "model.compile(\n",
        "    optimizer = # Not sure of the exact syntax but would like to use Adam and start training with the default values\n",
        "                #    __init__(\n",
        "                #    lr=0.001,\n",
        "                #    beta_1=0.9,\n",
        "                #    beta_2=0.999,\n",
        "                #    epsilon=None,\n",
        "                #    decay=0.0,\n",
        "                #    amsgrad=False,\n",
        "                #     ),\n",
        "    loss = \"sparse_categorical_crossentropy\",\n",
        "    metrics = [\"accuracy\"])\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f962fe2d6bef>\"\u001b[0;36m, line \u001b[0;32m30\u001b[0m\n\u001b[0;31m    tf.keras.layers.Conv1D(# because the input is sequential although not exactly temporal.\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "3KyQL32g1WuT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Defining inputs (pixel being an 1D array of a variable number of bands)\n",
        "tf.reset_default_graph()\n",
        "pixel=tf.placeholder(tf.float32, None, \"pixel\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iNiSma5T7n35",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Creating the graph\n",
        "# result = ..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iMMSdQRpS7pv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.4 Training the model\n",
        "Under construction since training is not accomplished yet\n"
      ]
    },
    {
      "metadata": {
        "id": "QZlN8MJj8vzx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Running the model\n",
        "session = tf.Session()\n",
        "writer=tf.summary.FileWriter(\"logs\",sesion.graph)\n",
        "session.run(tf.global_variables_initializer())\n",
        "session.run(result, feed_dict=(pixel=???))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tDgYGI5Q-Evh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "session.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cdeoS1HKTuWi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Related work, discussion\n",
        "\n",
        "The original article has over 220 citations and seens to have started a niche implementation model that was later on extended to 2D RNN also containing spatial information and 3D RNN containing temporal, spatial and spectral information ([Makantasis et al.](http://users.ntua.gr/karank/img/Makantasis_etal_igrass15.pdf),  [Kussul et al.](https://www.researchgate.net/profile/Andrey_Shelestov/publication/315939269_Deep_Learning_Classification_of_Land_Cover_and_Crop_Types_Using_Remote_Sensing_Data/links/5a0170914585159634c2f9f5/Deep-Learning-Classification-of-Land-Cover-and-Crop-Types-Using-Remote-Sensing-Data.pdf), [Li et al.](https://www.mdpi.com/2072-4292/9/1/67/htm) as quoted [here](https://arxiv.org/pdf/1710.03959.pdf)). \n",
        "\n",
        "Apart from the need for tuning, the model drafted in this assignment is likely to be buggy due to the need to consider the TF implementation of specific 1D dimensions (as explained [here](https://www.youtube.com/watch?v=ST73HFC4Lpo) for PyTorch) which need more detailed consideration. The laborious data processing that was one of the causes of slow and painful progress on the assignment suggests the need to re-implement the model with a sound data pipeline via the tf.Data interface once this becomes available. Such improvement would be especially interesting if the Sentnel Hub's API can be used for automating the server querying for products with specified characteristics (e.g. max. % of cloud coverage) or to monitor areas of interest. Some good scripts such as [Sentinelsat](https://sentinelsat.readthedocs.io/en/stable/index.html) are already available for automating tasks on the Sentinel Hub. \n",
        "\n",
        "Keeping track of dimensions is exacerbated also by the huge (by orders of magnitude) differences between the minimal and maximal potential length. Another interesting question is whether satellite images from different satellites but taken at approximately the same time may be concatenated to form a pixel image with a bigger number of channels. A huge question mark to such concatenation, however, is the uncertainty induced by changes in athmospheic and ground conditions with each satellite pass. \n",
        "\n",
        "A full bibliography of related work for the entire piprline is not discussed here for time constraints. Relater work for the current assignment was quoted inline above for better readability.\n",
        "\n",
        "***Acknowledgments***: Special thanks are due to the fabulous course trainer Yordan Darakchiev for answering all the questions I could think of, and then some :-) Also thanks to my classmate Vasil Vassilev for systematically widening my horizons on GIS and satellite imagery during the past few years, in particular in the scripting session of [last month's Copernicus workshop](https://cope4bg2018.copernicus.bg/). "
      ]
    }
  ]
}